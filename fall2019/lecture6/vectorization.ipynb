{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Для многих задач связанные с текстом нам бы хотелось представлять текст в виде вектора, чтобы похожие слова или предложения располагались рядом с друг другом. Для чего это нужно?. Например для задач поиска, нам хотелось бы выдавать ответ так, чтобы наиболее похожие статьи находились в верху списка. Такой тип представления текстов и слов называется дистрибутивная семантика.</p> \n",
    "<p>Каким же образом мы можем представить слова в таком виде?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Логичным методом представления слова является, контекст в котором оно употребляются в тексте</p>\n",
    "<p>Контекстом мы будем считать слова, которые находятся рядом с искомым словом в окне определенного размера, например для слова <i>лес</i> и окна 3, контекст выглядит следующим образом</p>\n",
    "<p>Житель <b>местности, покрытой хвойным</b> <i>лесом</i>, <b>связывает с «деревом»</b> прежде всего образ ели или сосны.</p>\n",
    "<p>Исходя из этой идеи, мы можем посчитать вектор слова следующим образом</p>\n",
    "\n",
    "- Посчитать слова которые встречаются с данным словом в одном контексте\n",
    "- Создать вектор где в определенное значение записана частота определенного слова из словаря\n",
    "\n",
    "<p>В таком случае похожесть между словами мы можем измерять с помощью косинусного расстояния между их векторами</p>\n",
    "<p>Однако такой способ имеет большой недостаток, а именно часто встречаемые слова (например предлоги) будут иметь очень большой вес. Чтобы уменьшить влияние таких слов, вместо частоты встречаемости слов, будем считать PMI (pointwise mutual information)</p>\n",
    "<p><center>$$\\large{PMI = \\log{\\frac{p(u, v)}{p(v)p(u)}}=\\log{\\frac{n_{uv}n}{n_un_v}}}$$</center></p>\n",
    "<p>Однако из с данным способом представления встречаемости слов есть проблема, для редко встречаемых между собой слов получается большое отрицательно значение, а если слова никогда не встречаются между собой то значение равно $-\\infty$. Поэтому на практике пользуются pPMI</p>\n",
    "<p><center>$$\\large{pPMI = \\max{(0, PMI)}}$$</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>И так у нас имеется способ представление совстречаемости слов с помощью pPMI. Так же мы можем мерить похожесть слов между собой с помощью косинусного расстояния. Однако у нас есть другая проблема а именно, размерность вектора каждого слова равна длинне словаря и затраты на вычисление похожести слов слишком большие. Поэтому воспользуюмся техникой снижения размерности а именно Truncated SVD</p>\n",
    "<p>Truncated SVD представляет нашу матрицу размера (W*W) в виде произведения двух матриц (W*K) и (K*W), где K - новая размерность для наших векторов</p>\n",
    "<img src=\"./img/pmi_svd.png\">\n",
    "<p>Для того что найти наилучшие матрицы для такого произведения мы решаем задачу минимизации расстояния между исходной матрицей и скалярного произведения новых матриц</p>\n",
    "<p><center>$\\large{||X-X_f||^2 \\rightarrow min}$</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>GloVe - Global Vectors for Word Representation алгоритм представления слов в виде векторов, был разработан в университет Стендфорд. Почитать о нем можно по <a href=\"https://nlp.stanford.edu/projects/glove/\">ссылке</a>, <a href=\"https://github.com/stanfordnlp/GloVe\">git</a> проекта где можно скачать вектора</p>\n",
    "<p>GloVe похож на вектора которые получаются с помощью SVD из pPMI матрицы, однако во время поиска матрицы представления слов, минимизируется другая ошибка и матрица заполнена логорифомом встречаемости слов:</p>\n",
    "<p><center>$$\\large{\\sum_{u \\in W}\\sum_{v \\in W}f(n_{uv})(\\phi_u\\theta_v + b_u + b_v - \\log{n_{uv}})^2}$$</center></p>\n",
    "<p>Это делается для того чтобы уменьшить влияние часто встречаемых слов</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>В данном случае мы пытаемся построить модель, которая бы предсказывала контекст по заданному слову:</p>\n",
    "<p><center>$$\\large{p(\\omega_{i-k}..\\omega_{i+k}|\\omega_{i}) = \\prod_{-k\\le j\\le k, j\\ne 0}p(\\omega_{i+j}|\\omega{i})}$$</center></p>\n",
    "<p>Где вероятность встречаемости каждого слова, представлена как:</p>\n",
    "<p><center>$$\\large{p(u|v) = \\frac{\\exp{\\phi_u\\theta_v}}{\\sum_{\\dot{u}\\in W}\\exp{\\phi_\\dot{u}\\theta_v}}}$$</center></p>\n",
    "<p>Так же как в версии с Truncated SVD у нас есть матрицы $\\Phi$ и $\\Theta$, в которых находятся векторы слов, в skip-gram мы максимизируем логарифмическое правдоподобие, вместо минимизации MSE</p>\n",
    "<p><center>$$\\large{L = \\sum_{u \\in W}\\sum_{v \\in W}n_{uv}\\log{p(u|v)}}$$</center></p>\n",
    "<p>Однако на практике данной моделью пользуются редко, поскольку ее обучение занимает очень много времени из-за того что нам нужно считать софтмакс по всему словарю</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Данная модель абсолютно аналогична Skip-gram за исключением того что функция максимизации состоит из 2 частей:</p>\n",
    "\n",
    "- для положительных примеров (слов из контекста) \n",
    "<p><center>$$\\large{L_{positive} = \\sum_{u \\in W}\\sum_{v \\in W}n_{uv}\\log{\\sigma(\\phi_u\\theta_v)}}$$</center></p>\n",
    "\n",
    "- для отрицательных примеров (слов вне контекста), выбираем случайные k слов \n",
    "<p><center>$$\\large{L_{negative} = kE_v\\log{\\sigma(-\\phi_u\\theta_v)}}$$</center></p>\n",
    "\n",
    "<p>Полная метрика выглядит как сумма позитивной и негативной метрик</p>\n",
    "<p><center>$$\\large L = L_{positive}+L_{negative}$$</center></p>\n",
    "<p>Интересным фактом является то что оптимальное значение для скалярного произведения skip-gram векторов является смещенная оценка PMI</p>\n",
    "<p><center>$$\\large{sPMI = PMI - \\log{k}}$$</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Архитектура CBOW аналогична Skip-gram, с одним отличием, мы пытаемся максимизировать вероятность найти слово по заданному контексту</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Doc2Vec это набор моделей которые к векторам слов, добавляют вектор документов для этих слов, сделанно это для того чтобы:</p>\n",
    "\n",
    "- Получить векторное представление документа\n",
    "- Использовать информацию о документе для того чтобы различать омонимы\n",
    "\n",
    "<p>В Doc2Vec представленно 2 модели</p>\n",
    "\n",
    "- Distributed memory version of Paragraph Vector\n",
    "\n",
    "<p><center>$$\\large{p(\\omega_{i}|\\omega_{i-k}..\\omega_{i+k}, d)}$$</center></p>\n",
    "\n",
    "- Distributed Bag of Words version of Paragraph Vector\n",
    "\n",
    "<p><center>$$\\large{p(\\omega_{i-k}..\\omega_{i+k}|d)}$$</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Skip-Gram, CBOW, Doc2Vec представлены в пакете <a href=\"https://radimrehurek.com/gensim/\">gensim</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Данная модель похожа на Skip-gram Negative Sampling, только вместо скалярного произведения вектора слов, мы используем скалярное произведение для сумм char-gram</p>\n",
    "<p>Пример 3char-gram для слова <i>семантика</i>: __с, _се, сем, ема, ман, ант, нти, тик, ика</p>\n",
    "<p><center>$$\\large{\\log{\\sigma(\\phi_u\\theta_v)} = \\log{\\sigma(\\sum_{c \\in word}\\phi_u c_v})}$$</center></p>\n",
    "<p><a href=\"https://github.com/facebookresearch/fastText/\">git FastText</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StarSpace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>В реальных задачах нам чаще нужны вектора целых предложений, вместо векторов слов нам нужны вектора целых предложений, существует несколько подходов для их получения</p>\n",
    "\n",
    "- среднее среди всех векторов в тексте\n",
    "- взвешенная сумма по idf векторов в тексте\n",
    "- пакет StarSpace\n",
    "\n",
    "<p>Пакет StarSpace можно найти на <a href=\"https://github.com/facebookresearch/StarSpace\">git</a>, идея данной модели очень схожа с идеей FastText только вместо похожести слов, мы ищем похожие предложения, каждое из которых представленно в виде сумм векторов слов. В результате мы получаем оптимизированные вектора слов для представления предложений</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_file = open('data/cc.ru.300.vec', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_file.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2000000 300\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(vector_file):\n",
    "    line = line.rstrip().split(' ')\n",
    "    words_dict[line[0]] = i\n",
    "    vectors.append(np.array([float(var) for var in line[1:]]).astype(np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.argsort(vectors.dot(vectors[words_dict['кролик']]))[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['пёс', 'бык', 'еж', 'ёж', 'Ёж']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k,v in words_dict.items() if v in j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.argsort(vectors.dot(vectors[words_dict['программирование']]))[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PHP', '1с', 'Qt', 'dq', 'UML']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k,v in words_dict.items() if v in j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
