{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf13759-df75-4d78-8ff4-63d4db49bdee",
   "metadata": {},
   "source": [
    "# Network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112de3fc-6f34-4637-a940-f0e486dcc8fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Softmax\n",
    "\n",
    "Итак мы выяснили как тренировать нашу нейронную сеть когда мы делаем предсказание регрессии или классификации на один класс, а что если классов несколько, например набор данных MNIST содержит 10 классов, по одному на каждую из цифр\n",
    "\n",
    "В этом случае, во-первых мы будем иметь несколько выходов из нашей сети, по одному на каждый класс\n",
    "\n",
    "<img src=\"img/dense.png\" style=\"background-color:white;\">\n",
    "\n",
    "Понятно что выходы могут иметь любое выходное значение, в зависимости от весов сети, однако мы хотели бы получить распеределение вероятностей для каждого из классов.\n",
    "\n",
    "Тогда нам нужно нормализовать выходы, чтобы их сумма равнялась 1, делают это с помощью Softmax\n",
    "\n",
    "$$\\large{S(z_j) =  \\frac{e^{z_j}}{\\sum_{i}^{C} e^{z_i}}}$$\n",
    "\n",
    "Где j - индекс искомого класса, C - количество классов в датасете\n",
    "\n",
    "Давайте рассмотрим градиент этой функции по z_k\n",
    "\n",
    "Для простоты вычислений воспользуемся небольшим трюком:\n",
    "\n",
    "$$\\large{\\frac{\\partial log(f(x))}{\\partial x} = \\frac{\\partial f(x)}{\\partial x} \\frac{1}{f(x)}}$$\n",
    "\n",
    "$$\\large{f(x)\\frac{\\partial log(f(x))}{\\partial x} = \\frac{\\partial f(x)}{\\partial x}}$$\n",
    "\n",
    "Раскроем логарифм поверх Softmax\n",
    "\n",
    "$$\\large{log(S(z_j))} = log(\\frac{e^{z_j}}{\\sum_{i}^{C} e^{z_i}}) = z_j - log(\\sum_{i}^{C} e^{z_i})))$$\n",
    "\n",
    "Производная от z_j равна 1 если j == k, иначе она равна нулю, запишем это через индексную функцию\n",
    "\n",
    "$$\\large{\\frac{\\partial log(S(z_j))}{\\partial z_k} = \\mathbb{1}[k == j] - \\frac{\\partial log(\\sum_{i}^{C} e^{z_i}))}{\\partial z_k}}$$\n",
    "\n",
    "Производная под логарифмом не нулевая только $e^{z_k}$\n",
    "\n",
    "$$\\large{\\frac{\\partial log(S(z_j))}{\\partial z_k} = \\mathbb{1}[k == j] - \\frac{e^{z_k}}{\\sum_{i}^{C} e^{z_i}}}$$\n",
    "\n",
    "Заметим что справа у нас опять стоит Softmax только на этот раз от k\n",
    "\n",
    "$$\\large{\\frac{\\partial log(S(z_j))}{\\partial z_k} = \\mathbb{1}[k == j] - S(z_k)}$$\n",
    "\n",
    "Возвратим градиент функции по трюку которым мы воспользовались\n",
    "\n",
    "$$\\large{\\frac{\\partial S(z_j)}{\\partial z_k} = S(z_j)(\\mathbb{1}[k == j] - S(z_k))}$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\large{\\frac{\\partial S(z_j)}{\\partial z_k} = \n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "      S(z_j)(1 - S(z_k)), & \\mbox{if k==j}\\\\\n",
    "      -S(z_j)S(z_k), & \\mbox{if k!=j}\n",
    "    \\end{cases}\\,.\n",
    "\\end{equation}\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "## 2. Cross Entropy\n",
    "\n",
    "Однако, нам так же нужно обновить и функцию потерь на многоклассовый случай. Воспользуемся выводами из BCE и применим их на многоклассовый выход\n",
    "\n",
    "В данном случае $y_i$ будет OHE вектором класса\n",
    "\n",
    "$$\\large{L = -\\sum_i^C y_i log(S(z_i))}$$\n",
    "\n",
    "Давайте посмотрим на градиент этой функции\n",
    "\n",
    "$$\\large{\\frac{\\partial L}{\\partial z_k} = -\\sum_i^C y_i \\frac{\\partial log(S(z_i))}{\\partial z_k}}$$\n",
    "\n",
    "$$\\large{\\frac{\\partial L}{\\partial z_k} = -\\sum_i^C y_i \\frac{\\partial S(z_i)}{\\partial z_k} \\frac{1}{S(z_i)}}$$\n",
    "\n",
    "Градиент по softmax мы вычислили ранее\n",
    "\n",
    "$$\\large{\\frac{\\partial L}{\\partial z_k} = -\\sum_i^C y_i S(z_i) (\\mathbb{1}[k == i] - S(z_k)) \\frac{1}{S(z_i)}}$$\n",
    "\n",
    "$$\\large{\\frac{\\partial L}{\\partial z_k} = -\\sum_i^C y_i (\\mathbb{1}[k == i] - S(z_k))}$$\n",
    "\n",
    "$$\\large{\\frac{\\partial L}{\\partial z_k} = -\\sum_i^C \\mathbb{1}[k == i] y_i +\\sum_i^C S(z_k) y_i}$$\n",
    "\n",
    "$$\\large{\\frac{\\partial L}{\\partial z_k} = -y_k + S(z_k) \\sum_i^C y_i}$$\n",
    "\n",
    "Воспользуймся фактом что сумма OHE векторов всех классов равна единичному\n",
    "\n",
    "$$\\large{\\frac{\\partial L}{\\partial z_k} = S(z_k) - y_k}$$\n",
    "\n",
    "Часто для удобства вычислений градиента Cross Entropy объеденяют с Softmax в одну функцию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068f623-84e1-4ad5-a22e-3f71f9ef016b",
   "metadata": {},
   "source": [
    "## 3. Градиентный спуск\n",
    "\n",
    "Вспомним как бы делали градиентный спуск в модели линейной регресии, мы расчитывали функцию потерь для всех входящих векторов и суммировали их\n",
    "\n",
    "Градиентный спуск был достаточно медленным алгоритмом, однако он хорошо искал минимум фукнции\n",
    "\n",
    "С другой стороны мы знаем про стохастический градиентный спуск, когда мы выбираем случайный вектор и считаем функцию потерь относительно него\n",
    "\n",
    "Стохастический градиентный спуск был очень быстрым алгоримом, но не всегда искал хороший минимум\n",
    "\n",
    "В данном курсе мы будем пользоваться средним вариантом Batch Gradient Descent - когда мы выбираем несколько случайных векторов и считаем функцию потерь относительно них\n",
    "\n",
    "<img src=\"img/bgd.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98374b5-c102-4d6d-8df9-cf401e1761d0",
   "metadata": {},
   "source": [
    "## 4. Оптимизация градиентного спуска\n",
    "\n",
    "### 4.1 Momentum\n",
    "\n",
    "![without_momentum](img/without_momentum.gif \"without_segment\")\n",
    "![with_momentum](img/with_momentum.gif \"with_segment\")\n",
    "\n",
    "Идея данной оптимизации состоит в том что мы сохраняем направление предыдущего шага и новый шаг делаем учитывая старое направление, таким образом мы уменьшаем влияние шумных признаков на оптимизацию\n",
    "\n",
    "$$\\large{\\upsilon_t = \\gamma \\upsilon_{t-1} + \\eta \\nabla_{W} J(W)}$$\n",
    "$$\\large{W = W - \\upsilon_t}$$\n",
    "\n",
    "$\\gamma$ - гиперпараметр алгорима, чем он больше, тем сильнее учитывается направление предыдущего шага, обычно $\\gamma = 0.9$\n",
    "\n",
    "Визуально можно представить шарик катящийся с горки по неровной дороге, он переодически отклоняется то влево то вправо, однако основное ускорение направленно вниз"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea6639-a7fe-4b72-b07e-c65aa7a75779",
   "metadata": {},
   "source": [
    "### 4.2 Nesterov accelerated gradient\n",
    "\n",
    "У метода momentum есть существенный недостаток, то что мы можем проскочить точку оптимума из-за того что двигались слишком быстро в направлении предыдущего шага, попробуем это решить \n",
    "\n",
    "$$\\large{\\upsilon_t = \\gamma \\upsilon_{t-1} + \\eta \\nabla_{W} J(W - \\gamma \\upsilon_{t-1})}$$\n",
    "$$\\large{W = W - \\upsilon_t}$$\n",
    "\n",
    "![nesterov](img/nesterov_update_vector.png)\n",
    "\n",
    "Итак momentum считает направление текущего градиента (маленький синий вектор), а после делает большой скачок в направлении предыдущего шага (большой коричневый вектор)\n",
    "\n",
    "NAG в свою очередь сначала делает скачок в направлении предущего шага, а после корректирует его (маленький красный вектор), что дает зеленый вектор\n",
    "\n",
    "Теперь, когда мы можем адаптировать наши обновления к наклону нашей функции ошибок и ускорить SGD, мы также хотели бы адаптировать наши обновления к каждому отдельному параметру, чтобы делать обновления в зависимости от их важности параметра."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265e320-a807-4fed-8513-1ff4b3e766d6",
   "metadata": {},
   "source": [
    "### 4.3 Adagrad\n",
    "\n",
    "Adagrad — это алгоритм оптимизации, который делает именно то что мы хотели: адаптирует скорость обучения к параметрам, выполняя небольшие обновления для параметров которые встречаются часто, и более крупные обновления для редко встречаемых параметров. Он делает это оптимизируя скорость обучения под параметры\n",
    "\n",
    "Итак определим $g_{t, i}$ как градиент в момент времени t для функции потерь по параметру сети i:\n",
    "\n",
    "$$\\large{g_{t,i} = \\nabla_WJ(W_{t,i})}$$\n",
    "\n",
    "Тогда обновления параметра сети в момент времени t, для SGD:\n",
    "\n",
    "$$\\large{\\theta_{t+1,i} = \\theta_{t,i} - \\eta g_{t, i}}$$\n",
    "\n",
    "Правило обновления параметров Adagrad изменяет скорость обучения на каждом шаге обновления, для каждого параметра, базируесь на прошлом значении градиента параметра:\n",
    "\n",
    "$$\\large{\\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\eta}{\\sqrt{G_{t,ii} + \\epsilon}} g_{t, i}}$$\n",
    "\n",
    "$G_{t,ii} \\in R ^ {d x d}$ -диагональная матрица где на диагонали находится сумма квадратов параметров $W_i$ в момент времени t, $\\epsilon$ - небольшая константа чтобы избежать деления на ноль, обычно 1e-8\n",
    "\n",
    "Чтобы избавится от номера параметра i перепишем в матричном виде\n",
    "\n",
    "$$\\large{\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{G_{t} + \\epsilon}} \\bigodot g_{t}}$$\n",
    "\n",
    "$\\bigodot$ - матрично-векторное произведение\n",
    "\n",
    "Главный недостаток Adagrad это накопление суммы градиента в знаменателе и чем дальше по времени мы движемся, тем более несущественными становятся наши обновления"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b8b93-a8ab-4a07-861c-20e72f3a9eea",
   "metadata": {},
   "source": [
    "### 4.4 Adadelta\n",
    "\n",
    "Adadelta - это улучшение Adagrad призванное уменьшить слишком быстрое снижение скорости обновления, adadelta сужает аккумулирование градиента вместо всей временной шкалы до окна $\\omega$\n",
    "\n",
    "Вместо неэффективного хранения значений $\\omega$ сумм предыдущих квадратов градиентов, мы будем считать экспонентоциальное среднее значений $E[g^2]_t$, в такой постановке наше значение $E[g^2]_t$ опирается лишь на текущий градиент и предыдущее значение:\n",
    "\n",
    "$$\\large{E[g^2]_{t} = \\gamma E[g^2]_{t - 1} + (1 - \\gamma) g^2_t}$$\n",
    "\n",
    "Мы будем использовать $\\gamma$ близкую к значение в momentum (около 0.9)\n",
    "\n",
    "Перепишем обновление SGD через $\\Delta W_t$\n",
    "\n",
    "$$\\large{\\Delta W_t} = -\\eta g_t$$\n",
    "\n",
    "$$\\large{W_{t+1}} = W_t + \\Delta W_t$$\n",
    "\n",
    "В Adagrad мы определили $\\Delta W_t$ как \n",
    "\n",
    "$$\\large{\\Delta W_t} = - \\frac{\\eta}{\\sqrt{G_{t} + \\epsilon}} \\bigodot g_{t}$$\n",
    "\n",
    "Теперь вместо $G_t$ мы используем $E[g^2]_t$\n",
    "\n",
    "$$\\large{\\Delta W_t} = - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_{t}$$\n",
    "\n",
    "Заметим что знаменатель это RMSE\n",
    "\n",
    "$$\\large{\\Delta W_t} = - \\frac{\\eta}{RMS[g_t]} g_{t}$$\n",
    "\n",
    "Авторы отмечают что в этом методе (а так же всех предыдущих), единицы величины части обновления отличаются от единиц параметра, чтобы исправить это они предложили высчитывать экспонентоциальное среднее изменения параметра а не самих параметров\n",
    "\n",
    "$$\\large{E[\\Delta W^2]_{t} = \\gamma E[\\Delta W^2]_{t - 1} + (1 - \\gamma) \\Delta W^2_t}$$\n",
    "\n",
    "$$\\large{RMS[\\Delta W_t] = \\sqrt{E[\\Delta W^2]_{t} + \\epsilon}}$$\n",
    "\n",
    "Поскольку $RMS[\\Delta W_t]$ нам неизвестен на шаге t мы апроксимируем его через RMS от обновления параметров на предыдущем шаге. Замета $\\eta$ в формуле обновления наконец то даст нам Adadelta\n",
    "\n",
    "$$\\large{\\Delta W_t} = - \\frac{RMS[\\Delta W_{t-1}]}{RMS[g_t]} g_{t}$$\n",
    "\n",
    "$$\\large{W_{t+1}} = W_t + \\Delta W_t$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c315b0-4c57-4138-8d66-0fbc7c9a0e3e",
   "metadata": {},
   "source": [
    "### 4.5 RMSprop\n",
    "\n",
    "RMSprop — это неопубликованный метод адаптивной скорости обучения.\n",
    "\n",
    "RMSprop и Adadelta были разработаны независимо друг от друга примерно в одно и то же время из-за необходимости решить проблему радикального снижения скорости обучения в Adagrad. Фактически RMSprop идентичен первому вектору обновления Adadelta, который мы получили выше:\n",
    "\n",
    "$$\\large{E[g^2]_{t} = \\gamma E[g^2]_{t - 1} + (1 - \\gamma) g^2_t}$$\n",
    "\n",
    "$$\\large{W_t} = W_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_{t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17574b7-6360-45dd-9c61-7f5294dbf5b4",
   "metadata": {},
   "source": [
    "### 4.6 Adam\n",
    "\n",
    "Adaptive Moment Estimation (Adam) — это еще один метод, который вычисляет скорость адаптивного обучения для каждого параметра. В дополнение к хранению экспоненциально затухающих средних значений прошлых квадратов градиентов $\\upsilon_t$ подобно Adadelta и RMSprop, Адам также хранит экспоненциально затухающие средние значения прошлых градиентов.\n",
    "\n",
    "$$\\large{m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t}$$\n",
    "$$\\large{\\upsilon_t = \\beta_2 \\upsilon_{t-1} + (1 - \\beta_2)g^2_t}$$\n",
    "\n",
    "$m_t$ и $\\upsilon_t$ приближают первый момент (среднее) и второй момент (смещенную дисперсию) параметров соотвественно\n",
    "\n",
    "Поскольку $m_t$ и $\\upsilon_t$ инициализируются как нулевые вектора, у нас имеется проблема что при больших значениях $\\beta_1$ и $\\beta_2$ $m_t$ и $\\upsilon_t$ в начальных шагах близки к нулю\n",
    "\n",
    "Поэтому будем использовать их исправленные версии\n",
    "\n",
    "$$\\large{\\hat{m_t} = \\frac{m_t}{1 - \\beta_1}}$$\n",
    "$$\\large{\\hat{\\upsilon_t} = \\frac{\\upsilon_t}{1 - \\beta_2}}$$\n",
    "\n",
    "Правило обновления схоже с Adadelta и RMSProp\n",
    "\n",
    "$$\\large{W_{t+1} = W_t - \\frac{\\eta}{\\sqrt{\\hat{\\upsilon_t}} + \\epsilon}\\hat{m_t}}$$\n",
    "\n",
    "Авторы метода рекомендуют использовать параметры $\\beta_1 = 0.9$, $\\beta_2 = 0.999$ и $\\epsilon = 1e-8$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
