{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для многих задач связанные с текстом нам бы хотелось представлять текст в виде вектора, чтобы похожие слова или предложения располагались рядом с друг другом. Для чего это нужно?. Например для задач поиска, нам хотелось бы выдавать ответ так, чтобы наиболее похожие статьи находились в верху списка. Такой тип представления текстов и слов называется дистрибутивная семантика. \n",
    "Каким же образом мы можем представить слова в таком виде?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логичным методом представления слова является, контекст в котором оно употребляются в тексте\n",
    "Контекстом мы будем считать слова, которые находятся рядом с искомым словом в окне определенного размера, например для слова <i>лес</i> и окна 3, контекст выглядит следующим образом\n",
    "Житель <b>местности, покрытой хвойным</b> <i>лесом</i>, <b>связывает с «деревом»</b> прежде всего образ ели или сосны.\n",
    "Исходя из этой идеи, мы можем посчитать вектор слова следующим образом\n",
    "\n",
    "- Посчитать слова которые встречаются с данным словом в одном контексте\n",
    "- Создать вектор где в определенное значение записана частота определенного слова из словаря\n",
    "\n",
    "В таком случае похожесть между словами мы можем измерять с помощью косинусного расстояния между их векторами\n",
    "Однако такой способ имеет большой недостаток, а именно часто встречаемые слова (например предлоги) будут иметь очень большой вес. Чтобы уменьшить влияние таких слов, вместо частоты встречаемости слов, будем считать PMI (pointwise mutual information)\n",
    "$$\\large{PMI = \\log{\\frac{p(u, v)}{p(v)p(u)}}=\\log{\\frac{n_{uv}n}{n_un_v}}}$$\n",
    "Однако из с данным способом представления встречаемости слов есть проблема, для редко встречаемых между собой слов получается большое отрицательно значение, а если слова никогда не встречаются между собой то значение равно $-\\infty$. Поэтому на практике пользуются pPMI\n",
    "$$\\large{pPMI = \\max{(0, PMI)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И так у нас имеется способ представление совстречаемости слов с помощью pPMI. Так же мы можем мерить похожесть слов между собой с помощью косинусного расстояния. Однако у нас есть другая проблема а именно, размерность вектора каждого слова равна длинне словаря и затраты на вычисление похожести слов слишком большие. Поэтому воспользуюмся техникой снижения размерности а именно Truncated SVD\n",
    "Truncated SVD представляет нашу матрицу размера (W*W) в виде произведения двух матриц (WK) и (KW), где K - новая размерность для наших векторов\n",
    "<img src=\"./img/pmi_svd.png\">\n",
    "Для того что найти наилучшие матрицы для такого произведения мы решаем задачу минимизации расстояния между исходной матрицей и скалярного произведения новых матриц\n",
    "$\\large{||X-X_f||^2 \\rightarrow min}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe - Global Vectors for Word Representation алгоритм представления слов в виде векторов, был разработан в университет Стендфорд. Почитать о нем можно по <a href=\"https://nlp.stanford.edu/projects/glove/\">ссылке</a>, <a href=\"https://github.com/stanfordnlp/GloVe\">git</a> проекта где можно скачать вектора\n",
    "GloVe похож на вектора которые получаются с помощью SVD из pPMI матрицы, однако во время поиска матрицы представления слов, минимизируется другая ошибка и матрица заполнена логорифомом встречаемости слов:\n",
    "$$\\large{\\sum_{u \\in W}\\sum_{v \\in W}f(n_{uv})(\\phi_u\\theta_v + b_u + b_v - \\log{n_{uv}})^2}$$\n",
    "Это делается для того чтобы уменьшить влияние часто встречаемых слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае мы пытаемся построить модель, которая бы предсказывала контекст по заданному слову:\n",
    "$$\\large{p(\\omega_{i-k}..\\omega_{i+k}|\\omega_{i}) = \\prod_{-k\\le j\\le k, j\\ne 0}p(\\omega_{i+j}|\\omega{i})}$$\n",
    "Где вероятность встречаемости каждого слова, представлена как:\n",
    "$$\\large{p(u|v) = \\frac{\\exp{\\phi_u\\theta_v}}{\\sum_{\\dot{u}\\in W}\\exp{\\phi_\\dot{u}\\theta_v}}}$$\n",
    "Так же как в версии с Truncated SVD у нас есть матрицы $\\Phi$ и $\\Theta$, в которых находятся векторы слов, в skip-gram мы максимизируем логарифмическое правдоподобие, вместо минимизации MSE\n",
    "$$\\large{L = \\sum_{u \\in W}\\sum_{v \\in W}n_{uv}\\log{p(u|v)}}$$\n",
    "Однако на практике данной моделью пользуются редко, поскольку ее обучение занимает очень много времени из-за того что нам нужно считать софтмакс по всему словарю"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данная модель абсолютно аналогична Skip-gram за исключением того что функция максимизации состоит из 2 частей:\n",
    "\n",
    "- для положительных примеров (слов из контекста) \n",
    "$$\\large{L_{positive} = \\sum_{u \\in W}\\sum_{v \\in W}n_{uv}\\log{\\sigma(\\phi_u\\theta_v)}}$$\n",
    "\n",
    "- для отрицательных примеров (слов вне контекста), выбираем случайные k слов \n",
    "$$\\large{L_{negative} = kE_v\\log{\\sigma(-\\phi_u\\theta_v)}}$$\n",
    "\n",
    "Полная метрика выглядит как сумма позитивной и негативной метрик\n",
    "$$\\large L = L_{positive}+L_{negative}$$\n",
    "Интересным фактом является то что оптимальное значение для скалярного произведения skip-gram векторов является смещенная оценка PMI\n",
    "$$\\large{sPMI = PMI - \\log{k}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архитектура CBOW аналогична Skip-gram, с одним отличием, мы пытаемся максимизировать вероятность найти слово по заданному контексту"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec это набор моделей которые к векторам слов, добавляют вектор документов для этих слов, сделанно это для того чтобы:\n",
    "\n",
    "- Получить векторное представление документа\n",
    "- Использовать информацию о документе для того чтобы различать омонимы\n",
    "\n",
    "В Doc2Vec представленно 2 модели\n",
    "\n",
    "- Distributed memory version of Paragraph Vector\n",
    "\n",
    "$$\\large{p(\\omega_{i}|\\omega_{i-k}..\\omega_{i+k}, d)}$$\n",
    "\n",
    "- Distributed Bag of Words version of Paragraph Vector\n",
    "\n",
    "$$\\large{p(\\omega_{i-k}..\\omega_{i+k}|d)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-Gram, CBOW, Doc2Vec представлены в пакете <a href=\"https://radimrehurek.com/gensim/\">gensim</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данная модель похожа на Skip-gram Negative Sampling, только вместо скалярного произведения вектора слов, мы используем скалярное произведение для сумм char-gram\n",
    "Пример 3char-gram для слова <i>семантика</i>: __с, _се, сем, ема, ман, ант, нти, тик, ика\n",
    "$$\\large{\\log{\\sigma(\\phi_u\\theta_v)} = \\log{\\sigma(\\sum_{c \\in word}\\phi_u c_v})}$$\n",
    "<a href=\"https://github.com/facebookresearch/fastText/\">git FastText</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StarSpace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реальных задачах нам чаще нужны вектора целых предложений, вместо векторов слов нам нужны вектора целых предложений, существует несколько подходов для их получения\n",
    "\n",
    "- среднее среди всех векторов в тексте\n",
    "- взвешенная сумма по idf векторов в тексте\n",
    "- пакет StarSpace\n",
    "\n",
    "Пакет StarSpace можно найти на <a href=\"https://github.com/facebookresearch/StarSpace\">git</a>, идея данной модели очень схожа с идеей FastText только вместо похожести слов, мы ищем похожие предложения, каждое из которых представленно в виде сумм векторов слов. В результате мы получаем оптимизированные вектора слов для представления предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/mlisuct/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apples', 0.720359742641449),\n",
       " ('pear', 0.6450697183609009),\n",
       " ('fruit', 0.6410146355628967),\n",
       " ('berry', 0.6302295327186584),\n",
       " ('pears', 0.613396167755127),\n",
       " ('strawberry', 0.6058261394500732),\n",
       " ('peach', 0.6025872230529785),\n",
       " ('potato', 0.5960935354232788),\n",
       " ('grape', 0.5935863852500916),\n",
       " ('blueberry', 0.5866668224334717)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674735069275),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.518113374710083),\n",
       " ('sultan', 0.5098593831062317),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# king - man + woman\n",
    "model.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModel(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def get_mean_vector(self, text):\n",
    "        v = np.zeros(300)\n",
    "        c = 0\n",
    "        for word in text.split(\" \"):\n",
    "            if word in self.model:\n",
    "                v += self.model.get_vector(word)\n",
    "                c += 1\n",
    "        c = max(1, c)\n",
    "        return v / c\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([self.get_mean_vector(x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2VecModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
