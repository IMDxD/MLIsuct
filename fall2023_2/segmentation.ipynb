{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc60b69-284f-4065-9244-0f406b8a5ef1",
   "metadata": {},
   "source": [
    "# Сегментация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a44655-c26d-48d3-a8bf-ca76b54ed4de",
   "metadata": {},
   "source": [
    "## Локализация\n",
    "\n",
    "Сегментация как и детекция является способом локализации объектов, но есть и отличия, если в детекции мы локализуем объект с помощью bounding boxes, то в сегментации мы каждому пикселю изображение ставим в соответствие его класс\n",
    "\n",
    "<img src=\"./img/segmentation_detection_cls.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2931ae-6701-49e0-a688-f71a80b5e42a",
   "metadata": {},
   "source": [
    "Сегментация используется вместо детекции используется всегда, когда нам важна площадь объекта, например\n",
    "\n",
    "- медицинские снимки <img src=\"./img/medical_seg.png\">\n",
    "\n",
    "- спутниковые снимки\n",
    "<img src=\"./img/map_seg.jpg\">\n",
    "\n",
    "- Или когда мы хотим поменять фон изображения\n",
    "\n",
    "<img src=\"./img/background_seg.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a67f3-764c-4707-b534-fea5b3bb8cff",
   "metadata": {},
   "source": [
    "## Виды сегментации\n",
    "\n",
    "- semantic segmentation - каждому пикселю (кроме фона) ставится в соответствие номер класса\n",
    "- instance segmentation - разделяются маски разных экземпляров одного класса\n",
    "- panoptic segmentation - объеденение instance segmentation для искомых объектов и semantic segmentation для фона\n",
    "\n",
    "<img src=\"./img/segmentation_types.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d1cbe-a433-4853-a975-1103c2bc4d32",
   "metadata": {},
   "source": [
    "## Semantic segmentation\n",
    "\n",
    "Семантическая сегментация аналогична попиксельной классификации\n",
    "\n",
    "- вход изображение\n",
    "- выход маска где каждому пикселю изображения соответствует какой либо класс\n",
    "\n",
    "Например, у нас имеется изображение, например машины и нужно отделить ее от фона\n",
    "\n",
    "<img src=\"./img/carvana.jpeg\">\n",
    "\n",
    "В маске всего 2 класса фон и объект\n",
    "\n",
    "<img src=\"./img/carvana_true.png\">\n",
    "\n",
    "Мы предсказываем маску с какой то вероятностью\n",
    "\n",
    "<img src=\"./img/carvana_pred.png\">\n",
    "\n",
    "Вопрос, какую функцию потерь использовать чтобы обучить модель?\n",
    "\n",
    "Первая мысль использовать стандартную Binary cross entropy в нашем примере или Cross entropy при мультиклассе\n",
    "\n",
    "$$\\large{BCE = -y log \\hat{y}} - (1 - y) log (1 - \\hat{y})$$\n",
    "\n",
    "Считаем лосс в каждом пикселе и усредняем по площади"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff878c1-d57b-43db-81ff-b57048c989a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d54d221a-4477-4b5e-8d7c-d44fdb518f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 640\n",
    "image_height = 480\n",
    "\n",
    "true_mask = torch.randint(0, 2, size=(image_height, image_width)).float()\n",
    "pred_mask = (1 - torch.randint(0, 2, size=(image_height, image_width))) * 0.4 + torch.randint(0, 2, size=(image_height, image_width)) * 0.6\n",
    "\n",
    "criteria = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97374532-b1ea-410e-b5b5-2a2281fe4df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.3415)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria(pred_mask, true_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c9a8a6-f3f5-4715-a5f3-46cc04705f09",
   "metadata": {},
   "source": [
    "Какие проблемы с таким подходом?\n",
    "\n",
    "<img src=\"./img/seg_size.png\">\n",
    "\n",
    "Аналогично задаче детекции, может быть сильный дисбаланс классов\n",
    "\n",
    "Возможные решения:\n",
    "- Focal loss\n",
    "- Взвешенный bce\n",
    "- Другие лоссы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98a2fe70-3756-461f-8ba6-2b818105e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha = None, # label smothing считаем разметку неточной\n",
    "            gamma = 2., # степерь гамма из фокал лосса\n",
    "            ignore_index = None, # какие классы мы игнорируем\n",
    "            reduction = \"mean\", # как аггрегируем функцию потерь\n",
    "            normalized = False, # нормировка по весам фокал лосса\n",
    "            reduced_threshold = None, # ограничение вероятности снизу для слишком редких объектов\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.ignore_index = ignore_index\n",
    "        self.focal_loss_fn = partial(\n",
    "            focal_loss,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            reduced_threshold=reduced_threshold,\n",
    "            reduction=reduction,\n",
    "            normalized=normalized,\n",
    "        )\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        num_classes = y_pred.size(1)\n",
    "        loss = torch.FloatTensor([0]).squeeze().to(y_pred.device)\n",
    "\n",
    "        # Filter anchors with -1 label from loss computation\n",
    "        not_ignored = None\n",
    "        if self.ignore_index is not None:\n",
    "            not_ignored = y_true != self.ignore_index\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            cls_y_true = (y_true == cls).long()\n",
    "            cls_y_pred = y_pred[:, cls, ...]\n",
    "\n",
    "            if self.ignore_index is not None:\n",
    "                cls_y_true = cls_y_true[not_ignored]\n",
    "                cls_y_pred = cls_y_pred[not_ignored]\n",
    "\n",
    "            loss += self.focal_loss_fn(cls_y_pred, cls_y_true)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def focal_loss(\n",
    "        output,\n",
    "        target,\n",
    "        gamma = 2.0,\n",
    "        alpha = 0.25,\n",
    "        reduction = \"mean\",\n",
    "        normalized = False,\n",
    "        reduced_threshold = None,\n",
    "        eps = 1e-6,\n",
    "):\n",
    "    target = target.type(output.type())\n",
    "\n",
    "    logpt = F.binary_cross_entropy_with_logits(output, target, reduction=\"none\")\n",
    "    pt = torch.exp(-logpt)\n",
    "\n",
    "    if reduced_threshold is None:\n",
    "        focal_term = (1.0 - pt).pow(gamma)\n",
    "    else:\n",
    "        focal_term = ((1.0 - pt) / reduced_threshold).pow(gamma)\n",
    "        focal_term[pt < reduced_threshold] = 1\n",
    "\n",
    "    loss = focal_term * logpt\n",
    "\n",
    "    if alpha is not None:\n",
    "        loss *= alpha * target + (1 - alpha) * (1 - target)\n",
    "\n",
    "    if normalized:\n",
    "        norm_factor = focal_term.sum().clamp_min(eps)\n",
    "        loss /= norm_factor\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        loss = loss.mean()\n",
    "    if reduction == \"sum\":\n",
    "        loss = loss.sum()\n",
    "    if reduction == \"batchwise_mean\":\n",
    "        loss = loss.sum(0)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a5f4ba2-eaca-4412-8b2b-95cea74db7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_pred = torch.cat([\n",
    "    1 - pred_mask.view(1, 1, image_height, image_width),\n",
    "    pred_mask.view(1, 1, image_height, image_width)\n",
    "], dim = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cb88e71-9031-421b-a0c3-f305f40a37c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "745e5359-2dc6-43b5-9b13-2f24de493f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7548)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(ce_pred, true_mask.view(1, image_height, image_width).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe327f95-419b-43e0-a4ef-32b945e64d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([0.45, 0.55]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5616395d-f380-4ba0-b6ac-ae44ed59fb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7549)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(ce_pred, true_mask.view(1, image_height, image_width).long())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b801fe-1c29-4270-ad20-482c8a2f8b89",
   "metadata": {},
   "source": [
    "## Архитектура модели\n",
    "\n",
    "Интерфейс\n",
    "- вход $H*W*C$\n",
    "- выход - К масок $H*W*C$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d85f0-dc81-408d-b1ea-f2eebc4e2967",
   "metadata": {},
   "source": [
    "### Fully-Convolutional Network (FCN) (2014)\n",
    "\n",
    "- Backbone (VGG)\n",
    "- Upsampling для приведения к исходному размеру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96ec450b-c8e8-4518-94ff-17dfa8cfbd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72210747-a796-4e8c-b204-2e8ecbd43401",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.rand(1, 3, 480, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fd55094-4c8a-46af-b7bf-23802253f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg16_bn().features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "133edd88-070c-4340-a1dc-285018aa6173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 15, 20])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9de980d9-f498-49f4-99c5-672366881c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32.0, 32.0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "640 / 20, 480 / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0119a49-00c5-4802-886f-f506b4a2be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.model = vgg16_bn().features\n",
    "        self.predictor = nn.Conv2d(512, n_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.predictor(x)\n",
    "        x = F.interpolate(x, scale_factor=32)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "25aced75-e0be-4b98-916c-f1b306692d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCN(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80405cc0-e1ee-404c-932b-36720d69bc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 480, 640])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f44f59-5cea-44a8-8278-d3942b8ac1a6",
   "metadata": {},
   "source": [
    "#### Проблемы\n",
    "\n",
    "- Узкое бутылочное горлышко\n",
    "- Резкое увеличение H/W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33cb02a-5a42-48f5-9f11-024a6684b269",
   "metadata": {},
   "source": [
    "### Interlude\n",
    "\n",
    "Как увеличить H/W?\n",
    "- Upsampling\n",
    "- Transposed convoulutions\n",
    "\n",
    "#### Upsampling\n",
    "\n",
    "- nearest neighbor\n",
    "\n",
    "<img src=\"./img/nn_upsample.png\">\n",
    "\n",
    "- bilinear\n",
    "\n",
    "<img src=\"./img/bilinear_upsample.webp\">\n",
    "\n",
    "<img src=\"./img/upsample_methods.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ad51b-12b2-4423-a5fc-3d3a55af21fa",
   "metadata": {},
   "source": [
    "## Transposed Conv\n",
    "\n",
    "Работает аналогично свертке но наоборот увеличивает изображение\n",
    "\n",
    "- Вместо \"сворачивания\" с ядром (как в обычном ConvLayer) происходит \"разворачивание\" входного сигнала\n",
    "- Значения входного сигнала выступают весами перед ядром транспонированной свертки\n",
    "\n",
    "<img src=\"./img/transp_conv.png\">\n",
    "\n",
    "- В некоторых случаях может появляться \"шахматный\" паттерн в выходном сигнале\n",
    "- <a href=\"https://distill.pub/2016/deconv-checkerboard/\">Deconvolution and Checkerboard Artifacts</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da21e232-6071-42e6-bb1b-1395d908b954",
   "metadata": {},
   "source": [
    "### SegNet (2015)\n",
    "\n",
    "- <a href=\"https://www.google.com/url?q=https://paperswithcode.com/paper/segnet-a-deep-convolutional-encoder-decoder&sa=D&source=editors&ust=1700561266684555&usg=AOvVaw3t3_Do6UypngMglu4zPJW0\">SegNet: A Deep Convolutional Encoder-Decoder Architecture</a>\n",
    "- Симметричная архитектура вида Encoder-Decoder\n",
    "- Постепенный Upsampling\n",
    "\n",
    "<img src=\"./img/segnet.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01707201-3a35-4f60-8b3e-17166b1af0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_chn=3, out_chn=32, BN_momentum=0.5):\n",
    "        super(SegNet, self).__init__()\n",
    "\n",
    "        #SegNet Architecture\n",
    "        #Takes input of size in_chn = 3 (RGB images have 3 channels)\n",
    "        #Outputs size label_chn (N # of classes)\n",
    "\n",
    "        #ENCODING consists of 5 stages\n",
    "        #Stage 1, 2 has 2 layers of Convolution + Batch Normalization + Max Pool respectively\n",
    "        #Stage 3, 4, 5 has 3 layers of Convolution + Batch Normalization + Max Pool respectively\n",
    "\n",
    "        #General Max Pool 2D for ENCODING layers\n",
    "        #Pooling indices are stored for Upsampling in DECODING layers\n",
    "\n",
    "        self.in_chn = in_chn\n",
    "        self.out_chn = out_chn\n",
    "\n",
    "        self.MaxEn = nn.MaxPool2d(2, stride=2, return_indices=True) \n",
    "\n",
    "        self.ConvEn11 = nn.Conv2d(self.in_chn, 64, kernel_size=3, padding=1)\n",
    "        self.BNEn11 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "        self.ConvEn12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.BNEn12 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.BNEn21 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "        self.ConvEn22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.BNEn22 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.BNEn31 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvEn32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNEn32 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvEn33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNEn33 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn41 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn42 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn43 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvEn51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn51 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn52 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvEn53 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNEn53 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "\n",
    "\n",
    "        #DECODING consists of 5 stages\n",
    "        #Each stage corresponds to their respective counterparts in ENCODING\n",
    "\n",
    "        #General Max Pool 2D/Upsampling for DECODING layers\n",
    "        self.MaxDe = nn.MaxUnpool2d(2, stride=2) \n",
    "\n",
    "        self.ConvDe53 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe53 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe52 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe51 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe43 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.BNDe42 = nn.BatchNorm2d(512, momentum=BN_momentum)\n",
    "        self.ConvDe41 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.BNDe41 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNDe33 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvDe32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.BNDe32 = nn.BatchNorm2d(256, momentum=BN_momentum)\n",
    "        self.ConvDe31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.BNDe31 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.BNDe22 = nn.BatchNorm2d(128, momentum=BN_momentum)\n",
    "        self.ConvDe21 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.BNDe21 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "\n",
    "        self.ConvDe12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.BNDe12 = nn.BatchNorm2d(64, momentum=BN_momentum)\n",
    "        self.ConvDe11 = nn.Conv2d(64, self.out_chn, kernel_size=3, padding=1)\n",
    "        self.BNDe11 = nn.BatchNorm2d(self.out_chn, momentum=BN_momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #ENCODE LAYERS\n",
    "        #Stage 1\n",
    "        x = F.relu(self.BNEn11(self.ConvEn11(x))) \n",
    "        x = F.relu(self.BNEn12(self.ConvEn12(x))) \n",
    "        x, ind1 = self.MaxEn(x)\n",
    "        size1 = x.size()\n",
    "\n",
    "        #Stage 2\n",
    "        x = F.relu(self.BNEn21(self.ConvEn21(x))) \n",
    "        x = F.relu(self.BNEn22(self.ConvEn22(x))) \n",
    "        x, ind2 = self.MaxEn(x)\n",
    "        size2 = x.size()\n",
    "\n",
    "        #Stage 3\n",
    "        x = F.relu(self.BNEn31(self.ConvEn31(x))) \n",
    "        x = F.relu(self.BNEn32(self.ConvEn32(x))) \n",
    "        x = F.relu(self.BNEn33(self.ConvEn33(x)))   \n",
    "        x, ind3 = self.MaxEn(x)\n",
    "        size3 = x.size()\n",
    "\n",
    "        #Stage 4\n",
    "        x = F.relu(self.BNEn41(self.ConvEn41(x))) \n",
    "        x = F.relu(self.BNEn42(self.ConvEn42(x))) \n",
    "        x = F.relu(self.BNEn43(self.ConvEn43(x)))   \n",
    "        x, ind4 = self.MaxEn(x)\n",
    "        size4 = x.size()\n",
    "\n",
    "        #Stage 5\n",
    "        x = F.relu(self.BNEn51(self.ConvEn51(x))) \n",
    "        x = F.relu(self.BNEn52(self.ConvEn52(x))) \n",
    "        x = F.relu(self.BNEn53(self.ConvEn53(x)))   \n",
    "        x, ind5 = self.MaxEn(x)\n",
    "        size5 = x.size()\n",
    "\n",
    "        #DECODE LAYERS\n",
    "        #Stage 5\n",
    "        x = self.MaxDe(x, ind5, output_size=size4)\n",
    "        x = F.relu(self.BNDe53(self.ConvDe53(x)))\n",
    "        x = F.relu(self.BNDe52(self.ConvDe52(x)))\n",
    "        x = F.relu(self.BNDe51(self.ConvDe51(x)))\n",
    "\n",
    "        #Stage 4\n",
    "        x = self.MaxDe(x, ind4, output_size=size3)\n",
    "        x = F.relu(self.BNDe43(self.ConvDe43(x)))\n",
    "        x = F.relu(self.BNDe42(self.ConvDe42(x)))\n",
    "        x = F.relu(self.BNDe41(self.ConvDe41(x)))\n",
    "\n",
    "        #Stage 3\n",
    "        x = self.MaxDe(x, ind3, output_size=size2)\n",
    "        x = F.relu(self.BNDe33(self.ConvDe33(x)))\n",
    "        x = F.relu(self.BNDe32(self.ConvDe32(x)))\n",
    "        x = F.relu(self.BNDe31(self.ConvDe31(x)))\n",
    "\n",
    "        #Stage 2\n",
    "        x = self.MaxDe(x, ind2, output_size=size1)\n",
    "        x = F.relu(self.BNDe22(self.ConvDe22(x)))\n",
    "        x = F.relu(self.BNDe21(self.ConvDe21(x)))\n",
    "\n",
    "        #Stage 1\n",
    "        x = self.MaxDe(x, ind1)\n",
    "        x = F.relu(self.BNDe12(self.ConvDe12(x)))\n",
    "        x = self.ConvDe11(x)\n",
    "\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "99855421-5bda-43cc-ada1-17275655da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegNet(3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b298592-d6e0-4ef4-8595-72818bdc36a8",
   "metadata": {},
   "source": [
    "### UNet(2015)\n",
    "\n",
    "- <a href=\"https://paperswithcode.com/paper/u-net-convolutional-networks-for-biomedical\">U-Net: CNNs for Biomedical Image Segmentation</a>\n",
    "- Добавили горизонтальные связи к Encoder-Decoder\n",
    "\n",
    "<img src=\"./img/unet.jpg\">\n",
    "\n",
    "- Сильное улучшение сегментации на границах объектов\n",
    "- Всего лишь 30 изображений 512х512 для обучения!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7faf2108-0713-41c3-8eee-e2cb530fefe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99afbe63-c43e-452c-ae3e-1c355f77e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0db59a6e-a835-432c-aa10-6133383341d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa129f64-c970-4458-8510-90ba8683b402",
   "metadata": {},
   "source": [
    "### UNet-like\n",
    "\n",
    "- Из конкретной архитектуры для сегментации UNet давно превратился в \"подход\" для задач image-to-image:\n",
    "-- Сегментация (subj)\n",
    "-- Колоризация (предсказание цветных каналов для grayscale-входа)\n",
    "-- InPainting (\"закрашивание\" пустот)\n",
    "-- SuperResolution (увеличение качества изображения)\n",
    "- UNet-like сеть = \n",
    "-- encoder (resnet, efficientnet, inception, …) + \n",
    "-- decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f928186-81ad-433f-82cf-0c2e3fca3df9",
   "metadata": {},
   "source": [
    "### Feature Pyramid Networks (FPN) (2017)\n",
    "\n",
    "- <a href=\"https://paperswithcode.com/paper/feature-pyramid-networks-for-object-detection\">Feature Pyramid Networks for Object Detection (2017)</a>\n",
    "- Та самая пирамида признаков, что использовали в RetinaNet\n",
    "- Идея подойдет и для улучшения сегментации\n",
    "\n",
    "<img src=\"./img/fpn.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c5ab7-cce4-4d5f-8716-dd75ea98e1c1",
   "metadata": {},
   "source": [
    "### Beyond BCE\n",
    "\n",
    "- Применение кросс-энтропии может сломаться о дисбаланс классов (и не только в сегментации)\n",
    "- Вспомним, что в детекторах объектов говорили про понятие Intersection-over-Union (IoU) (синоним - Jaccard Index):\n",
    "\n",
    "<img src=\"./img/iou.png\">\n",
    "<img src=\"./img/jaccard.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d737c86-8d11-4d1d-ad63-4990b842b181",
   "metadata": {},
   "source": [
    "- Напрямую оптимизировать Jaccard Index нельзя\n",
    "- Но можно аппроксимировать его, например:\n",
    "$$\\large{J_{seg} = \\frac{\\sum_{x, y} M_{gt}(x, y)M_{pred}(x, y)}{\\sum_{x, y} M_{gt}(x, y) + M_{pred}(x, y) - M_{gt}(x, y)M_{pred}(x, y)}}$$\n",
    "- Получить из этого лосс можно, например, так:\n",
    "$$\\large{Loss_J = 1 - log(J_{seg})}$$\n",
    "- Часто комбинируют с BCE:\n",
    "$$\\large{Loss = \\alpha Loss_{BCE} + (1 - \\alpha) Loss_J}$$\n",
    "- Jaccard Loss vs Dice Loss - постоянная <a href=\"https://stats.stackexchange.com/questions/381789/what-is-the-difference-between-dice-loss-vs-jaccard-loss-in-semantic-segmentatio\">путаница</a>\n",
    "- <a href=\"https://arxiv.org/pdf/2006.14822.pdf\">A survey of loss functions for semantic segmentation (2020)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6dcc01-1817-44c6-a4f2-90ee135807e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
